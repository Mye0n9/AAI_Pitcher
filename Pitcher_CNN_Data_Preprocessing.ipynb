{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1VXq_q28UA0biIBwNlKUrrISwDot0svMr",
      "authorship_tag": "ABX9TyOrjJidP3HCsf9L6QbDNa0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mye0n9/AAI_Pitcher/blob/CNN_Testing/Pitcher_CNN_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GkCgs-OWuzRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd600a7c-be2a-4e5d-81cb-eaa526798af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Version :1.22.4\n",
            "TensorFlow Version :2.12.0\n",
            "Matplotlib Version :3.7.1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as patches\n",
        "import glob\n",
        "import cv2\n",
        "import re\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "print(\"NumPy Version :{}\".format(np.__version__))\n",
        "print(\"TensorFlow Version :{}\".format(tf.__version__))\n",
        "print(\"Matplotlib Version :{}\".format(plt.matplotlib.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA_PREPROCESSING**"
      ],
      "metadata": {
        "id": "aVu_1nqwUsrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_data = []\n",
        "label_data = []\n",
        "# 9 folders and inside them are 33 folders\n",
        "for i in range(1,9):\n",
        "  for j in range(1,33):\n",
        "    image_link = '/content/drive/MyDrive/Raw_Data/{one}/{two}/*'.format(one = i,two = j)\n",
        "    label_link = '/content/drive/MyDrive/Label/{one}/{two}/*'.format(one = i,two = j)\n",
        "    image_data.append(glob.glob(image_link))\n",
        "    label_data.append(glob.glob(label_link))\n"
      ],
      "metadata": {
        "id": "0igFytKHUwdr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cropping wrist, elbow and shoulder of the image\n",
        "# 64 x 64\n",
        "def cropper (img, point, size):\n",
        "  # num1 is x\n",
        "  # num2 is y\n",
        "  max_x = 1920\n",
        "  max_y = 1080\n",
        "\n",
        "  x,y = point[0], point[1]\n",
        "  x1,x2,y1,y2 = 0,0,0,0\n",
        "\n",
        "  # x1 is smaller than x2\n",
        "  # y1 is smaller than y1\n",
        "\n",
        "  x1 = x-size\n",
        "  x2 = x+size\n",
        "  \n",
        "  y1 = y - size\n",
        "  y2 = y + size\n",
        "\n",
        "  cropped_img = img[y1:y2, x1:x2]\n",
        "  # print(cropped_img)\n",
        "  return cropped_img/255"
      ],
      "metadata": {
        "id": "I6FBRJo_1n50"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shoulders = []\n",
        "elbows = []\n",
        "wrists = []\n",
        "IMG_SIZE = 64\n",
        "\n",
        "# image_data and label_data not compatible\n",
        "vals_with_error = [96, 136, 197, 220, 223]\n",
        "\n",
        "for i in range(len(image_data)):\n",
        "  if i not in vals_with_error:\n",
        "    for j in range(len(image_data[i])):\n",
        "      f = open(label_data[i][j])\n",
        "      img = cv2.imread(image_data[i][j],cv2.IMREAD_COLOR)\n",
        "      data = json.load(f)\n",
        "      points = data['annotations'][1]['points']\n",
        "      # shoulder (5,6 -> 15,16 | 18,19)\n",
        "      shoulders.append(cropper(img,points[15:17],int(IMG_SIZE/2)))\n",
        "      shoulders.append(cropper(img,points[18:20],int(IMG_SIZE/2)))\n",
        "      # elbows (7,8 -> 21,22 | 24,25)\n",
        "      elbows.append(cropper(img,points[21:23],int(IMG_SIZE/2)))\n",
        "      elbows.append(cropper(img,points[24:26],int(IMG_SIZE/2)))\n",
        "      # wrists (9,10 -> 27,28 | 30, 31)\n",
        "      wrists.append(cropper(img,points[27:29],int(IMG_SIZE/2)))\n",
        "      wrists.append(cropper(img,points[30:32],int(IMG_SIZE/2)))\n",
        "      f.close()\n",
        "\n",
        "f_path = 'drive/MyDrive/'\n",
        "df = pd.DataFrame(data = [[wrists, elbows, wrists]], columns=['wrist', 'elbow','shoulder'])\n",
        "df.to_pickle(f_path + \"img_merged.pkl\")\n",
        "\n",
        "\n",
        "# total: 27278 images of wrists, elbows, and shoulders\n",
        "# takes 1hr 40min\n",
        "# About to use System RAM of Colab"
      ],
      "metadata": {
        "id": "dyV7qf42vMI9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking points\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# for i in shoulders:\n",
        "#   a = i.reshape(IMG_SIZE,IMG_SIZE,3)\n",
        "#   # print(i.index)\n",
        "#   # cv2_imshow(a)"
      ],
      "metadata": {
        "id": "gnCMzbUyiJtp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}